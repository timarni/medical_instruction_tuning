{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import interact_with_gpt_batch_version as script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 29\n"
     ]
    }
   ],
   "source": [
    "# Load file of batched prompts and check if it is the correct file:\n",
    "\n",
    "# Define the file path\n",
    "situations_name = \"specialty_x_domain\"\n",
    "turn = 3\n",
    "file_path = \"../results/batched_prompts_\" + situations_name + \"_turn_\" + str(turn) + \"c.jsonl\"\n",
    "\n",
    "# Initialize a counter\n",
    "count = 0\n",
    "\n",
    "# Open the file and count each line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        count += 1\n",
    "\n",
    "print(\"Number of entries:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to openai\n",
      "Creating batch file\n",
      "Batch file successfully created\n",
      "FileObject(id='file-jlVGMYsrJPltxkeImEQqgFY8', bytes=112678, created_at=1731699424, filename='batched_prompts_specialty_x_domain_turn_3c.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n",
      "Launch batch\n",
      "Batch successfully launched\n",
      "Batch id saved to ../results/batch_id.txt and batch object saved to ../results/batch.txt\n"
     ]
    }
   ],
   "source": [
    "# Launch batch job\n",
    "# Decomment line below to run job using openai API in batch mode\n",
    "# WILL COST MONEY\n",
    "# script.process_batch_by_gpt(path_to_batched_prompts = file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6737a2e135b88190932387383301a4b6', completion_window='24h', created_at=1731699425, endpoint='/v1/chat/completions', input_file_id='file-jlVGMYsrJPltxkeImEQqgFY8', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1731699495, error_file_id=None, errors=None, expired_at=None, expires_at=1731785825, failed_at=None, finalizing_at=1731699494, in_progress_at=1731699425, metadata={'description': 'generate multiturn interactions for medical instruction tuning'}, output_file_id='file-Li2IQ9oMMrCUdYQV5AzGvlop', request_counts=BatchRequestCounts(completed=29, failed=0, total=29))\n",
      "status = completed\n"
     ]
    }
   ],
   "source": [
    "# get status of batched job\n",
    "script.get_batch_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve results\n",
    "script.retrieve_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[Batch](data=[Batch(id='batch_6737a2e135b88190932387383301a4b6', completion_window='24h', created_at=1731699425, endpoint='/v1/chat/completions', input_file_id='file-jlVGMYsrJPltxkeImEQqgFY8', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1731699495, error_file_id=None, errors=None, expired_at=None, expires_at=1731785825, failed_at=None, finalizing_at=1731699494, in_progress_at=1731699425, metadata={'description': 'generate multiturn interactions for medical instruction tuning'}, output_file_id='file-Li2IQ9oMMrCUdYQV5AzGvlop', request_counts=BatchRequestCounts(completed=29, failed=0, total=29)), Batch(id='batch_6737a294d2d88190a1f0fdbb6b387397', completion_window='24h', created_at=1731699348, endpoint='/v1/chat/completions', input_file_id='file-T1BA3xMd3k80O1HtS8V5uz9K', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1731699358, error_file_id=None, errors=None, expired_at=None, expires_at=1731785748, failed_at=None, finalizing_at=1731699356, in_progress_at=1731699349, metadata={'description': 'generate multiturn interactions for medical instruction tuning'}, output_file_id='file-KVkIhuHNw7y2k3JphmkXKDJd', request_counts=BatchRequestCounts(completed=29, failed=0, total=29)), Batch(id='batch_673795e1a8c0819093b71c9fd4be8cd2', completion_window='24h', created_at=1731696097, endpoint='/v1/chat/completions', input_file_id='file-WKCLSibNTT1AHckSHON3hWnm', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1731697552, error_file_id=None, errors=None, expired_at=None, expires_at=1731782497, failed_at=None, finalizing_at=1731697301, in_progress_at=1731696104, metadata={'description': 'preprocess LLava-Pretrain dataset - part_1.jsonl'}, output_file_id='file-L9nV5SSgpjDigvgRIH7wA6xE', request_counts=BatchRequestCounts(completed=2861, failed=0, total=2861)), Batch(id='batch_673795c5e44481908ec20319957fd4e5', completion_window='24h', created_at=1731696069, endpoint='/v1/chat/completions', input_file_id='file-EhMlTt1Y7HIEq1Loe2Ip7Zo9', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1731696354, error_file_id=None, errors=None, expired_at=None, expires_at=1731782469, failed_at=None, finalizing_at=1731696349, in_progress_at=1731696071, metadata={'description': 'generate multiturn interactions for medical instruction tuning'}, output_file_id='file-W5OCMzR6te5uh1MeT0BRx6Zm', request_counts=BatchRequestCounts(completed=57, failed=0, total=57)), Batch(id='batch_673790d3644881909fbef48dbb4990cf', completion_window='24h', created_at=1731694803, endpoint='/v1/chat/completions', input_file_id='file-XM797QeQ2HlrpQmcXDhc1sU4', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1731694871, error_file_id=None, errors=None, expired_at=None, expires_at=1731781203, failed_at=None, finalizing_at=1731694866, in_progress_at=1731694803, metadata={'description': 'generate multiturn interactions for medical instruction tuning'}, output_file_id='file-eN4RSbTAxjk9aaBRdu36XoMA', request_counts=BatchRequestCounts(completed=57, failed=0, total=57)), Batch(id='batch_67378cbfd5b8819090d0f75a084d931f', completion_window='24h', created_at=1731693759, endpoint='/v1/chat/completions', input_file_id='file-kAivdwetJM2s6fV35JSbzbg3', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731780159, failed_at=1731693772, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_19.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378c8e23948190831fc9befc5f9132', completion_window='24h', created_at=1731693710, endpoint='/v1/chat/completions', input_file_id='file-7u1zlyHZ3FvSODsQOGaWUbqn', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731780110, failed_at=1731693724, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_18.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378c5e0a3c819084542c7a91116b01', completion_window='24h', created_at=1731693662, endpoint='/v1/chat/completions', input_file_id='file-9G37cdTfQWUcd3SoDf2VtWpQ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731780062, failed_at=1731693676, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_17.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378c2d15a08190873a0b1aed58c39f', completion_window='24h', created_at=1731693613, endpoint='/v1/chat/completions', input_file_id='file-QiGG4corFsNpnMTXYL1hVWny', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731780013, failed_at=1731693636, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_16.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378bf977588190aab98e9d605ed47e', completion_window='24h', created_at=1731693562, endpoint='/v1/chat/completions', input_file_id='file-FPbOFv3MabffhUaLIsyxD0LA', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779962, failed_at=1731693642, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_15.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378bc560c08190adc841e8d4165827', completion_window='24h', created_at=1731693509, endpoint='/v1/chat/completions', input_file_id='file-TMTsQDCY559p5ZYnxhoiJljL', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779909, failed_at=1731693531, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_14.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378b921b4081909db3331569373cbe', completion_window='24h', created_at=1731693458, endpoint='/v1/chat/completions', input_file_id='file-ItiYvFabekKKHyTPuJRVSYi8', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779858, failed_at=1731693472, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_13.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378b64221481909c7c39872fe729cb', completion_window='24h', created_at=1731693412, endpoint='/v1/chat/completions', input_file_id='file-5gfXT8MbsnpqAynsoMYvLu8C', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779812, failed_at=1731693428, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_12.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378b3632b08190a255cf726d913543', completion_window='24h', created_at=1731693366, endpoint='/v1/chat/completions', input_file_id='file-6FChFxYo25vfDY9YB6OeIxnq', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779766, failed_at=1731693380, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_11.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378b05af9081908f044275e46cbe56', completion_window='24h', created_at=1731693317, endpoint='/v1/chat/completions', input_file_id='file-XPHByAtGIPzqCIFKl5KwEoAi', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779717, failed_at=1731693330, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_10.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378ad3aaf48190aa923b9c7f430e0c', completion_window='24h', created_at=1731693267, endpoint='/v1/chat/completions', input_file_id='file-uzdDVQ86lXBPHO36hXSLlIUm', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779667, failed_at=1731693287, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_9.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378aa47c24819098c1ed96975a5886', completion_window='24h', created_at=1731693220, endpoint='/v1/chat/completions', input_file_id='file-Lo0WQPWOBzlQy2OrwOtcxRf2', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779620, failed_at=1731693235, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_8.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378a781dec8190938e84cdeb03dc8b', completion_window='24h', created_at=1731693176, endpoint='/v1/chat/completions', input_file_id='file-BsuET6YAUbj2YcCLbDpXLH86', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779576, failed_at=1731693188, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_7.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378a42b3a481909194c42d9b34c814', completion_window='24h', created_at=1731693122, endpoint='/v1/chat/completions', input_file_id='file-gZP7Tn1fQuTxg2n4CAkIbUU0', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779522, failed_at=1731693134, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_6.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)), Batch(id='batch_67378a106da481908d4ddc58aa297f26', completion_window='24h', created_at=1731693072, endpoint='/v1/chat/completions', input_file_id='file-XFN7aJqoCZ1ZsOrPkkE0RNTK', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1731779472, failed_at=1731693086, finalizing_at=None, in_progress_at=None, metadata={'description': 'preprocess LLava-Pretrain dataset - part_5.jsonl'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))], object='list', first_id='batch_6737a2e135b88190932387383301a4b6', last_id='batch_67378a106da481908d4ddc58aa297f26', has_more=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of jobs\n",
    "from openai import OpenAI\n",
    "\n",
    "path_to_api_key: str = \"../API_KEY.txt\"\n",
    "\n",
    "my_api_key = open(path_to_api_key, 'r').read()\n",
    "client = OpenAI(api_key=my_api_key)\n",
    "\n",
    "\n",
    "client.batches.list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihlqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

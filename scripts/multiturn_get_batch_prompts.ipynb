{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates batches for Multiturn chat creations\n",
    "\n",
    "* Cell 0: import necesary libraries\n",
    "* Cell 1: Create system prompts for user and chatbot (called situations) and store them for later use -> do not redo that!\n",
    "* Cell 2: Create file that stores the multiturn conversation and add the initial prompt to it -> do not redo that\n",
    "* Cell 3: Helper function to build prompt from chat history\n",
    "* Cell 4: Creates batches of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiturn_modul import MultiturnStyle, get_multiturn_style, get_multiturn_style_additional, find_profession_by_specialty\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating list of situations and initial prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20395/20395 [00:06<00:00, 3189.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to D-2586-6B-Burkina Faso prompts with situations saved to ../results/situations_task_x_specialties_x_demographic_x_answerstyle_2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create situations\n",
    "# Creates randomly system prompts for the user and the chatbot\n",
    "\n",
    "# Path for input data and to store prompts\n",
    "initial_prompts_name = \"task_x_specialties_x_demographic_x_answerstyle\"\n",
    "initial_prompts_path = \"../results/parsed_prompts_\" + initial_prompts_name + \".json\"\n",
    "output_path = \"../results/situations_\" + initial_prompts_name + \".jsonl\"\n",
    "\n",
    "# store initial prompts in a list\n",
    "with open(initial_prompts_path, 'r') as file:\n",
    "    initial_prompts_list = json.load(file)  # Load JSON data from file\n",
    "\n",
    "# Load countries necessary to give the medical setting\n",
    "countries = pd.read_csv(\"../resources/countries_by_income_category.csv\")\n",
    "\n",
    "print(\"Creating list of situations and initial prompts\")\n",
    "\n",
    "with open(output_path, 'w') as file: ### only remove if the situations are new and the situations file is never used anywhere\n",
    "    for initial_prompt in tqdm(initial_prompts_list):\n",
    "        sampled_country = random.choice(countries.iloc[:, 0].tolist())\n",
    "        id = initial_prompt[\"id\"]\n",
    "        if initial_prompts_name == \"specialty_x_domain\":\n",
    "            profession, specialty, domain = initial_prompt[\"context\"].lower().split(\",\")\n",
    "            multiturn_style = get_multiturn_style(id=id, sampled_country=sampled_country, profession=profession, specialty=specialty, domain=domain)\n",
    "            print(\"error\")\n",
    "        elif initial_prompts_name == \"task_x_specialties_x_demographic_x_answerstyle_2\":\n",
    "            context = initial_prompt[\"context\"]\n",
    "            multiturn_style = get_multiturn_style_additional(id=id, context=context)\n",
    "        else:\n",
    "            multiturn_style = get_multiturn_style(id=id, sampled_country=sampled_country)\n",
    "            print(\"error\")\n",
    "\n",
    "        system_prompt_chatbot = multiturn_style.system_prompt_chatbot()\n",
    "        system_prompt_user = multiturn_style.system_prompt_user()\n",
    "        nbr_of_turns = multiturn_style.number_of_turns\n",
    "        line = {\n",
    "            \"id\": id,\n",
    "            \"nbr_of_turns\": nbr_of_turns,\n",
    "            \"initial_prompt\": initial_prompt[\"prompt\"],\n",
    "            \"system_prompt_chatbot\": system_prompt_chatbot,\n",
    "            \"system_prompt_user\": system_prompt_user,\n",
    "            \"multiturn_style_parameters\": multiturn_style.get_all_system_prompts_atributes()\n",
    "        }\n",
    "        file.write(json.dumps(line) + '\\n')\n",
    "\n",
    "print(f\"Up to {id} prompts with situations saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20395/20395 [00:00<00:00, 95590.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file to store the multiturn conversations for task_x_specialties_x_demographic_x_answerstyle_2 has been created\n",
      "The file contains the initial prompt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create initial multiturn_situation .jsonl\n",
    "# Function to get the initial prompt\n",
    "\n",
    "situations_name = \"task_x_specialties_x_demographic_x_answerstyle_2\"\n",
    "\n",
    "situations_path = \"../results/situations_\" + situations_name + \".jsonl\"\n",
    "output_path = \"../results/multiturn_\" + situations_name + \".jsonl\"\n",
    "\n",
    "multiturn_conversations = []\n",
    "with ###open(situations_path, 'r') as file: ### do only exectue if it is the first time\n",
    "    for entry in file:\n",
    "        data = json.loads(entry)\n",
    "        line = {\n",
    "            \"id\": data[\"id\"],\n",
    "            \"conversation\": [\n",
    "                {\"role\": \"user\", \"value\": data[\"initial_prompt\"]},\n",
    "            ]\n",
    "        }\n",
    "        multiturn_conversations.append(line)\n",
    "# Save to JSON file\n",
    "with open(output_path, 'w') as jsonl_file:\n",
    "    for conv in tqdm(multiturn_conversations):\n",
    "        jsonl_file.write(json.dumps(conv) + '\\n')\n",
    "\n",
    "\n",
    "print(f\"The file to store the multiturn conversations for {situations_name} has been created\")\n",
    "print(f\"The file contains the initial prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(conv_list):\n",
    "    prompt = []\n",
    "    for entry in conv_list:\n",
    "        prompt.append(f\"({entry[\"role\"]}) {entry[\"value\"]} \\n\")\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a batch of prompts that can be processed by gpt-4o in batch mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20395/20395 [00:00<00:00, 28415.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch of 20395 prompts saved to ../results/batched_prompts_task_x_specialties_x_demographic_x_answerstyle_2_turn_1c.jsonl\n",
      "IMPORTANT: batches were created for turn 1 and chatbot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create batch of prompts\n",
    "\n",
    "# Specify which model should be used to answer prompts\n",
    "gpt_model = \"gpt-4o\"\n",
    "\n",
    "# Path for input data and to store prompts\n",
    "situations_name = \"task_x_specialties_x_demographic_x_answerstyle_2\" # Change this\n",
    "situations_path = \"../results/situations_\" + situations_name + \".jsonl\"\n",
    "multiturn_conv_path = \"../results/multiturn_\" + situations_name + \".jsonl\"\n",
    "turn = 1 # can be 1, 2 or 3 # Change this\n",
    "system_is = \"chatbot\" # can be user or chatbot # Change this\n",
    "output_path = \"../results/batched_prompts_\" + situations_name + \"_turn_\" + str(turn) + system_is[0] + \".jsonl\"\n",
    "\n",
    "\n",
    "# store prompts in a list\n",
    "situations = []\n",
    "with open(situations_path, 'r') as file:\n",
    "    for i, line in enumerate(file): # i could be used in test mode with if i >= 100; break\n",
    "        situations.append(json.loads(line))\n",
    "\n",
    "# store multiturn convs in a dict\n",
    "multiturn_conv = {}\n",
    "with open(multiturn_conv_path, 'r') as file:\n",
    "    for i, line in enumerate(file): # can use i if we want to test -> if i >= 100; break\n",
    "        line_as_json = json.loads(line)\n",
    "        multiturn_conv[line_as_json[\"id\"]] = line_as_json[\"conversation\"]\n",
    "\n",
    "\n",
    "print(f\"Creating a batch of prompts that can be processed by {gpt_model} in batch mode\")\n",
    "\n",
    "with open(output_path, 'w') as file:\n",
    "    nbr_of_prompts = 0\n",
    "    for situation in tqdm(situations):\n",
    "        if situation[\"nbr_of_turns\"] >= turn:\n",
    "            line = {\n",
    "                \"custom_id\": situation[\"id\"],\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": gpt_model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": situation[\"system_prompt_\" + system_is]},\n",
    "                        {\"role\": \"user\", \"content\": build_prompt(multiturn_conv[situation[\"id\"]])}\n",
    "                    ],\n",
    "                    \"max_tokens\": 1000\n",
    "                }\n",
    "            }\n",
    "            file.write(json.dumps(line) + '\\n')\n",
    "            nbr_of_prompts += 1\n",
    "\n",
    "print(f\"batch of {nbr_of_prompts} prompts saved to {output_path}\")\n",
    "print(f\"IMPORTANT: batches were created for turn {turn} and {system_is}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihlqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 -> Multiturn Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiturn_modul import MultiturnStyle, get_multiturn_style\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating list of situations and initial prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6321/6321 [00:01<00:00, 4430.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to 20-6320 prompts with situations saved to ../results/situations_specialty_x_domain.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create situations\n",
    "\n",
    "# Path for input data and to store prompts\n",
    "initial_prompts_name = \"specialty_x_domain\"\n",
    "initial_prompts_path = \"../results/parsed_prompts_\" + initial_prompts_name + \".json\"\n",
    "output_path = \"../results/situations_\" + initial_prompts_name + \".jsonl\"\n",
    "\n",
    "\n",
    "# store prompts in a list\n",
    "with open(initial_prompts_path, 'r') as file:\n",
    "    initial_prompts_list = json.load(file)  # Load JSON data from file\n",
    "\n",
    "# Load countries necessary to give the medical setting\n",
    "countries = pd.read_csv(\"../resources/countries_by_income_category.csv\")\n",
    "\n",
    "\n",
    "print(\"Creating list of situations and initial prompts\")\n",
    "\n",
    "with open(output_path, 'w') as file:\n",
    "    for initial_prompt in tqdm(initial_prompts_list):\n",
    "        sampled_country = random.choice(countries.iloc[:, 0].tolist())\n",
    "        id = initial_prompt[\"id\"]\n",
    "        if initial_prompts_name == \"specialty_x_domain\":\n",
    "            profession, specialty, domain = initial_prompt[\"context\"].lower().split(\",\")\n",
    "            multiturn_style = get_multiturn_style(id=id, sampled_country=sampled_country, profession=profession, specialty=specialty, domain=domain)\n",
    "        else:\n",
    "            multiturn_style = get_multiturn_style(id=id, sampled_country=sampled_country)\n",
    "        system_prompt_chatbot = multiturn_style.system_prompt_chatbot()\n",
    "        system_prompt_user = multiturn_style.system_prompt_user()\n",
    "        nbr_of_turns = multiturn_style.number_of_turns\n",
    "        line = {\n",
    "            \"id\": id,\n",
    "            \"nbr_of_turns\": nbr_of_turns,\n",
    "            \"initial_prompt\": initial_prompt[\"prompt\"],\n",
    "            \"system_prompt_chatbot\": system_prompt_chatbot,\n",
    "            \"system_prompt_user\": system_prompt_user,\n",
    "            \"multiturn_style_parameters\": multiturn_style.get_all_system_prompts_atributes()\n",
    "        }\n",
    "        file.write(json.dumps(line) + '\\n')\n",
    "\n",
    "print(f\"Up to {id} prompts with situations saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a batch of prompts\n",
      "that can be processed by gpt-4o in batch mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 49461.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch of 100 prompts saved to ../results/batched_prompts_specialty_x_domain.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create batch of prompts\n",
    "\n",
    "# Specify which model should be used to answer prompts\n",
    "gpt_model = \"gpt-4o\"\n",
    "\n",
    "# Path for input data and to store prompts\n",
    "situations_name = \"specialty_x_domain\"\n",
    "situations_path = \"../results/situations_\" + situations_name + \".jsonl\"\n",
    "output_path = \"../results/batched_prompts_\" + situations_name + \".jsonl\"\n",
    "\n",
    "\n",
    "# store prompts in a list\n",
    "situations = []\n",
    "with open(situations_path, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        situations.append(json.loads(line))\n",
    "\n",
    "print(\"Creating a batch of prompts\")\n",
    "print(f\"that can be processed by {gpt_model} in batch mode\")\n",
    "\n",
    "with open(output_path, 'w') as file:\n",
    "    nbr_of_prompts = 0\n",
    "    for situation in tqdm(situations):\n",
    "        line = {\n",
    "            \"custom_id\": situation[\"id\"],\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": gpt_model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": situation[\"system_prompt_chatbot\"]},\n",
    "                    {\"role\": \"user\", \"content\": situation[\"initial_prompt\"]}\n",
    "                ],\n",
    "                \"max_tokens\": 1000\n",
    "            }\n",
    "        }\n",
    "        file.write(json.dumps(line) + '\\n')\n",
    "        nbr_of_prompts += 1\n",
    "\n",
    "print(f\"batch of {nbr_of_prompts} prompts saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihlqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
